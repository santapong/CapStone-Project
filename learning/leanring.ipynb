{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangChain is an open-source project that aims to provide a universal interface for interacting with web3 data and smart contracts, allowing developers to build applications that seamlessly connect to various blockchain networks.\\n\\nTo better understand LangChain, let's break it down into its core components:\\n\\n1. **LangChain Core**: This is the foundation of the platform, which provides a set of libraries and tools for interacting with web3 data and smart contracts. It includes modules for working with NFTs, tokens, and more.\\n2. **LangChain Bridge**: This component enables seamless interaction between different blockchain networks, allowing developers to build decentralized applications (dApps) that work across multiple chains.\\n3. **LangChain SDKs**: LangChain provides software development kits (SDKs) for popular programming languages like Rust, JavaScript, and Python, making it easier for developers to integrate the platform into their existing projects.\\n4. **LangChain CLI**: The command-line interface (CLI) allows users to interact with LangChain directly from the terminal, making it easy to manage and deploy smart contracts, NFTs, and other web3 assets.\\n\\nBy providing a unified interface for interacting with web3 data and smart contracts, LangChain aims to simplify the development process for decentralized applications and make it more accessible to a wider range of developers.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_350813/163044844.py:24: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain: LLMChain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
      "/tmp/ipykernel_350813/163044844.py:27: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  prediction_msg: dict = chain.run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "The correct translation of \"I love Pizza!\" in French is:\n",
      "\n",
      "\"J'aime le pizza!\"\n",
      "\n",
      "However, if you want to express a stronger affection or enthusiasm for pizza, you can use the more informal expression:\n",
      "\n",
      "\"Je suis fou/folle de pizza!\"\n",
      "\n",
      "This translates to \"I'm crazy/obsessed with pizza!\"\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "# Create the first prompt template\n",
    "sys_prompt: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"original_sentence\", \"desired_language\"],\n",
    "    template=\"\"\"You are a language translater, an English speaker wants to translate/\n",
    "    {original_sentence} to {desired_language}. Tell him the corrent answer.\"\"\"\n",
    ")\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate(prompt=sys_prompt)\n",
    "\n",
    "student_prompt: PromptTemplate = PromptTemplate(\n",
    "    input_variables=[\"original_sentence\", \"desired_language\"],\n",
    "    template=\"Translate {original_sentence} to {desired_language}\"\n",
    ")\n",
    "student_message_prompt = HumanMessagePromptTemplate(prompt=student_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, student_message_prompt])\n",
    "\n",
    "# create the chat model\n",
    "chat_model = OllamaLLM(model='llama3.2')\n",
    "\n",
    "\n",
    "# Create the LLM chain\n",
    "chain: LLMChain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
    "\n",
    "# make a call to the models\n",
    "prediction_msg: dict = chain.run(\n",
    "    original_sentence=\"I love Pizza!\", desired_language=\"French\")\n",
    "\n",
    "print(\"#######################################\")\n",
    "print(prediction_msg)\n",
    "print(\"#######################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
